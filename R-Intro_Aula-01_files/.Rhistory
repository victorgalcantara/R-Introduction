# Factor: isso também não tem no py! ;)
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom")
)
# Factor: isso também não tem no py! ;)
a <- 1:4
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom")
)
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
# Factor: isso também não tem no py! ;)
a <- c("Péssimo","Ruim","Regular","Bom")
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
# Factor: isso também não tem no py! ;)
a <- c("Péssimo","Ruim","Regular","Bom")
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
a[1]
# Factor: isso também não tem no py! ;)
a <- sample(x=c("Péssimo","Ruim","Regular","Bom"),replace=T,size = 100)
# Factor: isso também não tem no py! ;)
a <- sample(x=c("Péssimo","Ruim","Regular","Bom"),replace=T,size = 100)
a
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
# Factor: isso também não tem no py! ;)
a <- sample(x=c(1:4),replace=T,size = 100)
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
mean(a)
class(a)
a <- sample(x=c("Péssimo","Ruim","Regular","Bom"),replace=T,size = 100)
a <- factor(a,
levels=c("Péssimo","Ruim","Regular","Bom"),
labels=c(1:4),
ordered=T
)
a
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a <- sample(x=c("Péssimo","Ruim","Regular","Bom"),replace=T,size = 100)
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
levels(a)
a <- factor(a, levels = c("Péssimo","Ruim","Regular","Bom"), ordered = TRUE)
min(a)
# Factor: isso também não tem no py! ;)
a <- c("Péssimo","Ruim","Regular","Bom")
a <- factor(a)
a
levels(a)
a <- factor(a, levels = c("Péssimo","Ruim","Regular","Bom"), ordered = TRUE)
levels(a)
mean(a)
levels(a)
a
min(a)
max(a)
mean(a)
a <- factor(a, levels = 1:4, ordered = TRUE)
mean(a)
a
# Factor: isso também não tem no py! ;)
a <- c("Péssimo","Ruim","Regular","Bom")
a <- factor(a)
levels(a)
a <- factor(a, levels = c("Péssimo","Ruim","Regular","Bom"), ordered = TRUE)
levels(a)
a
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
mean(a)
# Factor: isso também não tem no py! ;)
a <- c("Péssimo","Ruim","Regular","Bom")
a <- factor(a,
levels=c(1:4),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
# Factor: isso também não tem no py! ;)
a <- c("Péssimo","Ruim","Regular","Bom")
a <- factor(a,
levels=c("Péssimo","Ruim","Regular","Bom"),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
min(a)
max(a)
mean(a)
# Factor: isso também não tem no py! ;)
a <- c("Bom","Ruim","Regular","Péssimo")
a <- factor(a,
levels=c("Péssimo","Ruim","Regular","Bom"),
labels=c("Péssimo","Ruim","Regular","Bom"),
ordered=T
)
a
min(a)
max(a)
mean(a)
levels(a)
a
levels(a)
min(a)
max(a)
idad  <- c(65,74,21,24,27,34,24,23)
educ  <- c(2,2,12,14,12,16,14,14)
rend  <- c(1200,1600,1800,2200,1800,6400,2300,2500)
sexo  <- c("M","F","F","M","F","M","F","M")
raca  <- c("Branca","Parda","Preta","Parda","Preta","Branca",
"Parda","Amarela")
d <- data.frame(educ = educ, idad = idad, rend = rend,
sexo = sexo, raca = raca)
class(d)  # classe do objeto
dim(d)    # dimensão da base (linhas x colunas)
head(d)   # Primeiros 5 casos
tail(d)   # Últimos 5 casos
str(d)    # Estrutura dos dados
summary(d)      # sumário das variáveis
summary(d$rend) # sumário de uma variável
summary(d$sexo) # sumário de uma variável
# Subset: como acessar os dados [linhas,colunas]
d[1,1]
subset(x = d,subset = sexo == "F")
subset(x = d, select = "sexo")
page <- read_html(url)
library(rvest)     # R Easily Harvest
url <- "https://www.al.sp.gov.br/alesp/pesquisa-proposicoes/?method=search&natureId=1&legislativeNumber=&legislativeYear=&author=&text=sociologia&__ncforminfo=QPayYD5fvda9q4IFwz0FCdd7u-SflFwzVyZfAFeTa8z8VAr10u7p5kLfYuyVcC7WZ0hGGK1n8Fr9_vK_RGUfs6IvpDl5bYSJMV3nYURN91kyGa-B2lU1-fq_QJpEt2c-z9NCyrfdk9deSkNU5SPjv-2C-7rz1Im2hiFSY-_ep6Ex7dHT4bFc1MeWSmMizvDNwlwCsihXiB9b6Xx8A-_osKq8bjxvqG8eLD5uHwBv8c2n8gGNc42tgI1OMfJlQUjOZkoDMLM3bl7c2nozLkWO97LhHkh0X5yoC4jCKqM_8ziapWfXnS-sPtguCf2O1eWJsLg8eBfBvXc%3D"
page <- read_html(url)
class(page) # Xml: eXtensible Markup Language
tabelas <- html_table(x = page,header = T,fill=T)
str(tabelas)
# 4. Formatando tabela para estrutura data.frame (survey)
mydata <- tabelas[[1]]
str(mydata)
mydata <- as.data.frame(mydata)
str(mydata)
# 5. Limpando a base
# Para cada célula vazia, coloque NA
mydata$Autor <- ifelse(mydata$Autor == "",NA,mydata$Autor)
# Com R base
mydata[mydata[,1]=="",] <- NA
# Com tidyverse
mydata <- mutate(mydata,
Autor = ifelse(Autor == "",NA,Autor))
# Tudo isso para?
mydata <- na.exclude(mydata)
mydata <- separate(data=mydata,
col="Documento",
into=c("projeto","data-proposta"),
sep=c(",")
)
mydata <- separate(data=mydata,
col="data-proposta",
into=c("dm","data","proposta"),
sep=c(3,14,max(nchar(mydata$`data-proposta`)))
)
library(tidyverse) # Conj. de pacotes manuseio dados
mydata <- separate(data=mydata,
col="Documento",
into=c("projeto","data-proposta"),
sep=c(",")
)
mydata <- separate(data=mydata,
col="data-proposta",
into=c("dm","data","proposta"),
sep=c(3,14,max(nchar(mydata$`data-proposta`)))
)
mydata
View(mydata)
# Removendo coluna
mydata <- mydata[,-3]
# Extraindo infos sobre a data
mydata$proposta <- str_remove_all(mydata$proposta,pattern = "\r|\t|\n")
View(mydata)
url
# 2. Data management -------------------------------------------------------
t <- html_nodes(page, xpath = "//a")
t
html_attr(x = t, name = "href")
# 1. Import databases --------------------------------------------------------
# url: "http://publicacoes.fcc.org.br/index.php/eae/issue/archive"
url <- "http://anpocs.org/index.php/publicacoes-sp-2056165036/rbcs?limitstart=0"
# Lendo o documento html
links <- read_html(url) %>%
html_nodes(xpath = "//h1[@class='title']/a") %>%
html_attr(name = "href")
# p?gina da Revista n
tag_a_link_art <- read_html(paste0("http://anpocs.org",links_nrev[1]) ) %>%
html_nodes(xpath = "//a")
# p?gina da Revista n
tag_a_link_art <- read_html(paste0("http://anpocs.org",links[1]) ) %>%
html_nodes(xpath = "//a")
text  <- html_text(tag_a_link_art, trim = T)  %>% str_replace_all(.,c("/"),"")
links <- text == "pdf"
links_art <- html_attr(tag_a_link_art[links], name = "href")
links_art
links <- NULL
ed <- NULL
for(i in 1:4){
# URL de cada página
url <-  paste0("http://publicacoes.fcc.org.br/index.php/cp/issue/archive/",i)
pagina <- read_html(url)
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_edicoes   <- html_nodes(pagina, xpath = "//a[@class='title']")
tag_div_edicoes <- html_nodes(pagina, xpath = "//div[@class='series']")
# extraindo das tags selecionadas os links no atributo "href"
links_edicoes <- html_attr(tag_a_edicoes, name = "href")
# textos com descr das edicoes
text_edicoes  <- html_text(tag_a_edicoes, trim = T)  %>% str_replace_all(.,c("/"),"")
if(i == 1){
text_edicoes2 <- html_text(tag_div_edicoes,trim = T) %>% str_replace_all(.,c("/"),"")
text_edicoes[3:33] <- paste(text_edicoes2,text_edicoes[3:33])
}
# Guardando infos em cada página
ed    <- c(ed,text_edicoes)
links <- c(links,links_edicoes)
}
for(i in 2:10){
# Edicoes da revista I -------
# lendo o doc html da revista edicao x
ed_i <- read_html(links[i])
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_ed_i  <- html_nodes(ed_i, xpath = "//h3[@class='title']/a")
# extraindo das tags selecionadas os links no atributo "href"
links_artigos <- html_attr(tag_a_ed_i, name = "href")
titulos       <- html_text(tag_a_ed_i,trim = T) # extrai o texto na tag, no caso o título do art
# Guarda link dos artigos
link_artigo = NULL
for(j in 1:length(links_artigos)) { # Artigos J --------------------
print(j)
# pag de descrição do art
page_descricao_artigo <- read_html(links_artigos[j])
# localizando a tag <a> </a> com o link para a pag do artigo
tag_a_art  <- html_nodes(page_descricao_artigo, xpath = "//a[@class='obj_galley_link pdf']")
# link da pag do artigo
link_page_artigo <- html_attr(tag_a_art, name = "href")
# pag do artigo
page_artigo <- read_html(link_page_artigo[1])
# tag download
tag <- html_nodes(page_artigo, xpath = "//a[@class='download']")
# link do download
link_artigo[j] <- html_attr(tag[1], name = "href")
}
dir.create(text_edicoes[i])
artigos <- data.frame("id"=c(1:length(links_artigos)),"titulo"=titulos,"link-download"=link_artigo,
"link-artigo"=links_artigos)
write.csv(artigos,paste0(text_edicoes[i],"/catalogo"),
row.names = F)
for(j in 1:length(link_artigo)){
download.file(link_artigo[j],
paste0(text_edicoes[i],"/",j,".pdf"), # caminho do arquivo
mode = "wb")}
}
getwd()
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_edicoes   <- html_nodes(pagina, xpath = "//a[@class='title']")
# URL de cada página
url <-  paste0("http://publicacoes.fcc.org.br/index.php/cp/issue/archive/",i)
pagina <- read_html(url)
pagina <- read_html(url)
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_edicoes   <- html_nodes(pagina, xpath = "//a[@class='title']")
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_edicoes   <- html_nodes(pagina, xpath = "//a[@class='title']")
tag_div_edicoes <- html_nodes(pagina, xpath = "//div[@class='series']")
# extraindo das tags selecionadas os links no atributo "href"
links_edicoes <- html_attr(tag_a_edicoes, name = "href")
# textos com descr das edicoes
text_edicoes  <- html_text(tag_a_edicoes, trim = T)  %>% str_replace_all(.,c("/"),"")
if(i == 1){
text_edicoes2 <- html_text(tag_div_edicoes,trim = T) %>% str_replace_all(.,c("/"),"")
text_edicoes[3:33] <- paste(text_edicoes2,text_edicoes[3:33])
}
# Guardando infos em cada página
ed    <- c(ed,text_edicoes)
links <- c(links,links_edicoes)
links
for(i in 2:6){
# Edicoes da revista I -------
# lendo o doc html da revista edicao x
ed_i <- read_html(links[i])
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_ed_i  <- html_nodes(ed_i, xpath = "//h3[@class='title']/a")
# extraindo das tags selecionadas os links no atributo "href"
links_artigos <- html_attr(tag_a_ed_i, name = "href")
titulos       <- html_text(tag_a_ed_i,trim = T) # extrai o texto na tag, no caso o título do art
# Guarda link dos artigos
link_artigo = NULL
for(j in 1:length(links_artigos)) { # Artigos J --------------------
print(j)
# pag de descrição do art
page_descricao_artigo <- read_html(links_artigos[j])
# localizando a tag <a> </a> com o link para a pag do artigo
tag_a_art  <- html_nodes(page_descricao_artigo, xpath = "//a[@class='obj_galley_link pdf']")
# link da pag do artigo
link_page_artigo <- html_attr(tag_a_art, name = "href")
# pag do artigo
page_artigo <- read_html(link_page_artigo[1])
# tag download
tag <- html_nodes(page_artigo, xpath = "//a[@class='download']")
# link do download
link_artigo[j] <- html_attr(tag[1], name = "href")
}
dir.create(text_edicoes[i])
artigos <- data.frame("id"=c(1:length(links_artigos)),"titulo"=titulos,"link-download"=link_artigo,
"link-artigo"=links_artigos)
write.csv(artigos,paste0(text_edicoes[i],"/catalogo"),
row.names = F)
for(j in 1:length(link_artigo)){
download.file(link_artigo[j],
paste0(text_edicoes[i],"/",j,".pdf"), # caminho do arquivo
mode = "wb")}
}
length(links_artigos)
for(j in 1:3) { # Artigos J --------------------
print(j)
# pag de descrição do art
page_descricao_artigo <- read_html(links_artigos[j])
# localizando a tag <a> </a> com o link para a pag do artigo
tag_a_art  <- html_nodes(page_descricao_artigo, xpath = "//a[@class='obj_galley_link pdf']")
# link da pag do artigo
link_page_artigo <- html_attr(tag_a_art, name = "href")
# pag do artigo
page_artigo <- read_html(link_page_artigo[1])
# tag download
tag <- html_nodes(page_artigo, xpath = "//a[@class='download']")
# link do download
link_artigo[j] <- html_attr(tag[1], name = "href")
}
getwd()
links <- NULL
ed <- NULL
for(i in 1:4){
# URL de cada página
url <-  paste0("http://publicacoes.fcc.org.br/index.php/cp/issue/archive/",i)
pagina <- read_html(url)
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_edicoes   <- html_nodes(pagina, xpath = "//a[@class='title']")
tag_div_edicoes <- html_nodes(pagina, xpath = "//div[@class='series']")
# extraindo das tags selecionadas os links no atributo "href"
links_edicoes <- html_attr(tag_a_edicoes, name = "href")
# textos com descr das edicoes
text_edicoes  <- html_text(tag_a_edicoes, trim = T)  %>% str_replace_all(.,c("/"),"")
if(i == 1){
text_edicoes2 <- html_text(tag_div_edicoes,trim = T) %>% str_replace_all(.,c("/"),"")
text_edicoes[3:33] <- paste(text_edicoes2,text_edicoes[3:33])
}
# Guardando infos em cada página
ed    <- c(ed,text_edicoes)
links <- c(links,links_edicoes)
}
class(ed)
ed <- ed[1:3]
links <- links[1:3]
for(i in 2:6){
# Edicoes da revista I -------
# lendo o doc html da revista edicao x
ed_i <- read_html(links[i])
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_ed_i  <- html_nodes(ed_i, xpath = "//h3[@class='title']/a")
# extraindo das tags selecionadas os links no atributo "href"
links_artigos <- html_attr(tag_a_ed_i, name = "href")
titulos       <- html_text(tag_a_ed_i,trim = T) # extrai o texto na tag, no caso o título do art
# Guarda link dos artigos
link_artigo = NULL
for(j in 1:3) { # Artigos J --------------------
print(j)
# pag de descrição do art
page_descricao_artigo <- read_html(links_artigos[j])
# localizando a tag <a> </a> com o link para a pag do artigo
tag_a_art  <- html_nodes(page_descricao_artigo, xpath = "//a[@class='obj_galley_link pdf']")
# link da pag do artigo
link_page_artigo <- html_attr(tag_a_art, name = "href")
# pag do artigo
page_artigo <- read_html(link_page_artigo[1])
# tag download
tag <- html_nodes(page_artigo, xpath = "//a[@class='download']")
# link do download
link_artigo[j] <- html_attr(tag[1], name = "href")
}
dir.create(text_edicoes[i])
artigos <- data.frame("id"=c(1:length(links_artigos)),"titulo"=titulos,"link-download"=link_artigo,
"link-artigo"=links_artigos)
write.csv(artigos,paste0(text_edicoes[i],"/catalogo"),
row.names = F)
for(j in 1:length(link_artigo)){
download.file(link_artigo[j],
paste0(text_edicoes[i],"/",j,".pdf"), # caminho do arquivo
mode = "wb")}
}
ed_i <- read_html(links[i])
ed_i
i
for(i in 1:3){
# Edicoes da revista I -------
# lendo o doc html da revista edicao x
ed_i <- read_html(links[i])
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_ed_i  <- html_nodes(ed_i, xpath = "//h3[@class='title']/a")
# extraindo das tags selecionadas os links no atributo "href"
links_artigos <- html_attr(tag_a_ed_i, name = "href")
titulos       <- html_text(tag_a_ed_i,trim = T) # extrai o texto na tag, no caso o título do art
# Guarda link dos artigos
link_artigo = NULL
for(j in 1:3) { # Artigos J --------------------
print(j)
# pag de descrição do art
page_descricao_artigo <- read_html(links_artigos[j])
# localizando a tag <a> </a> com o link para a pag do artigo
tag_a_art  <- html_nodes(page_descricao_artigo, xpath = "//a[@class='obj_galley_link pdf']")
# link da pag do artigo
link_page_artigo <- html_attr(tag_a_art, name = "href")
# pag do artigo
page_artigo <- read_html(link_page_artigo[1])
# tag download
tag <- html_nodes(page_artigo, xpath = "//a[@class='download']")
# link do download
link_artigo[j] <- html_attr(tag[1], name = "href")
}
dir.create(text_edicoes[i])
artigos <- data.frame("id"=c(1:length(links_artigos)),"titulo"=titulos,"link-download"=link_artigo,
"link-artigo"=links_artigos)
write.csv(artigos,paste0(text_edicoes[i],"/catalogo"),
row.names = F)
for(j in 1:length(link_artigo)){
download.file(link_artigo[j],
paste0(text_edicoes[i],"/",j,".pdf"), # caminho do arquivo
mode = "wb")}
}
ed <- ed[1:2]
links <- links[1:2]
for(i in 1:3){
# Edicoes da revista I -------
# lendo o doc html da revista edicao x
ed_i <- read_html(links[i])
# navegando no documento: extraindo as tags "a" com class='cover'
tag_a_ed_i  <- html_nodes(ed_i, xpath = "//h3[@class='title']/a")
# extraindo das tags selecionadas os links no atributo "href"
links_artigos <- html_attr(tag_a_ed_i, name = "href")
titulos       <- html_text(tag_a_ed_i,trim = T) # extrai o texto na tag, no caso o título do art
# Guarda link dos artigos
link_artigo = NULL
for(j in 1:length(links_artigos)) { # Artigos J --------------------
print(j)
# pag de descrição do art
page_descricao_artigo <- read_html(links_artigos[j])
# localizando a tag <a> </a> com o link para a pag do artigo
tag_a_art  <- html_nodes(page_descricao_artigo, xpath = "//a[@class='obj_galley_link pdf']")
# link da pag do artigo
link_page_artigo <- html_attr(tag_a_art, name = "href")
# pag do artigo
page_artigo <- read_html(link_page_artigo[1])
# tag download
tag <- html_nodes(page_artigo, xpath = "//a[@class='download']")
# link do download
link_artigo[j] <- html_attr(tag[1], name = "href")
}
dir.create(text_edicoes[i])
artigos <- data.frame("id"=c(1:length(links_artigos)),"titulo"=titulos,"link-download"=link_artigo,
"link-artigo"=links_artigos)
write.csv(artigos,paste0(text_edicoes[i],"/catalogo"),
row.names = F)
for(j in 1:length(link_artigo)){
download.file(link_artigo[j],
paste0(text_edicoes[i],"/",j,".pdf"), # caminho do arquivo
mode = "wb")}
}
# Navegação no computador ----
dir()
# Navegação no computador ----
getwd()
dir()
dir.create("minha pasta")
# Navegação no computador ----
getwd()
